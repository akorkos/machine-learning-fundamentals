{"cells":[{"cell_type":"markdown","metadata":{"id":"cQ7YzabwjTmk"},"source":["## Assignment 3 ##\n","\n","Welcome to your third assignment. This assignment aims to help you understand \n","ensemble models."]},{"cell_type":"markdown","metadata":{"id":"egArYhcsTG-T"},"source":["**1** Read the available breast cancer dataset from sklearn, split it into \n","training data (X_train, y_train) and test data (X_test, y_test) with a split \n","ratio of 70%/30% using the train_test_split function (set random_state to 0). \n","The dataset concerns the diagnosis of breast cancer based on variables computed \n","from a digitized image of a fine needle aspirate (FNA) of a breast mass sample. "]},{"cell_type":"code","execution_count":86,"metadata":{"deletable":false,"id":"qgaPtNAmTCX7","nbgrader":{"cell_type":"code","checksum":"29b99b6039fac516d5fa9c7649e40e1d","grade":false,"grade_id":"cell-407a2dea48bfbf80","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","\n","cancer_data = load_breast_cancer()\n","X = cancer_data.data\n","y = cancer_data.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n","                                                            random_state=0)"]},{"cell_type":"code","execution_count":87,"metadata":{"deletable":false,"editable":false,"id":"19LlgZx5cOLP","nbgrader":{"cell_type":"code","checksum":"e4b7add4e469cfc1221bdad01497d404","grade":true,"grade_id":"cell-7387a72f2393ed9a","locked":true,"points":2,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["\"\"\"Τεστ ορθής ανάγνωσης και διαχωρισμού του συνόλου δεδομένων\"\"\"\n","assert round(X_train[0][8], 5) == 0.1779\n","assert round(X_test[0][8], 5) == 0.2116"]},{"cell_type":"markdown","metadata":{"id":"RB8RexuPciQr"},"source":["**2** Implement a deterministic version of the Random Subspaces method, which \n","builds as many models as there are input variables, each ignoring a different \n","input variable. For example, the first model ignores the first variable, the \n","second model ignores the second variable, and so on. Use the clone function \n","from sklearn.base to create a copy of the base model in each iteration."]},{"cell_type":"code","execution_count":88,"metadata":{"deletable":false,"id":"KuC_s04KcigR","nbgrader":{"cell_type":"code","checksum":"5fbe3d9c3d8e46ffc9d9cf06a7644fa3","grade":false,"grade_id":"cell-df57dc0d540a2518","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["import numpy as np\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.base import clone\n","\n","class RandomSubspaceDet:\n","    def __init__(self, estimator=DecisionTreeClassifier()):\n","        self.estimator = estimator\n","        self.estimators = []\n","\n","    def fit(self, X_train, y_train):\n","        N = X_train.shape[1]\n","\n","        for n in range(N):\n","            estimator = clone(self.estimator)\n","            X_train_reduced = np.delete(X_train, n, axis=1)\n","            estimator.fit(X_train_reduced, y_train)\n","            self.estimators.append(estimator)\n","\n","    def predict(self, X):\n","        n = X.shape[0]\n","        m = len(self.estimators)\n","        predictions = np.zeros((n, m))\n","\n","        for j, estimator in enumerate(self.estimators):\n","            X_reduced = np.delete(X, j, axis=1)\n","            predictions[:, j] = estimator.predict(X_reduced)\n","\n","        return np.round(np.mean(predictions, axis=1))"]},{"cell_type":"code","execution_count":89,"metadata":{"deletable":false,"editable":false,"id":"iDNUeGUEciwi","nbgrader":{"cell_type":"code","checksum":"4db933425279be3712175509b4ba2f53","grade":true,"grade_id":"cell-786f87fa5e67b624","locked":true,"points":4,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["\"\"\"Τεστ ορθής υλοποίησης RandomSubspaceDet\"\"\"\n","from sklearn.metrics import accuracy_score\n","\n","rs = RandomSubspaceDet(estimator=DecisionTreeClassifier(random_state=1))\n","rs.fit(X_train, y_train)\n","assert round(accuracy_score(rs.predict(X_test), y_test), 4) == 0.9006"]},{"cell_type":"markdown","metadata":{"id":"n19eEYRNRnG-"},"source":["**3** Implement the AdaBoost method as presented in the lesson. \n","Use the clone function from sklearn.base to create a copy of the base model in \n","each iteration. Utilize the sample_weight parameter of the base model's fit \n","method to set the weights of the training examples. "]},{"cell_type":"code","execution_count":90,"metadata":{"deletable":false,"id":"7NOoKPI8TBX6","nbgrader":{"cell_type":"code","checksum":"7fe3d39eaf3d6a53e2b29f9ebd1e7b6a","grade":false,"grade_id":"cell-946d2440bc05714e","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["import numpy as np\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.base import clone\n","\n","class AdaBoost:\n","    def __init__(self, n_estimators=20, \n","                estimator=DecisionTreeClassifier(max_depth=1)):\n","        self.n_estimators = n_estimators\n","        self.estimator = estimator\n","        self.estimator_weights = np.zeros(self.n_estimators)\n","        self.estimator_errors = np.ones(self.n_estimators)\n","        self.estimators = []\n","\n","    def fit(self, X_train, y_train):\n","        self.classes = np.array(sorted(list(set(y_train))))\n","        self.n_classes = len(self.classes)\n","\n","        m = X_train.shape[0]\n","        weight = np.ones(m) / m\n","\n","        def boost(X, y, weight):\n","            estimator = clone(self.estimator)\n","            estimator.fit(X, y, sample_weight=weight)\n","            y_pred = estimator.predict(X)\n","\n","            misses = y_pred != y\n","\n","            estimator_error = np.dot(weight, misses) / np.sum(weight)\n","            estimator_weight = np.log((1 - estimator_error) / estimator_error)\n","\n","            weight *= np.exp(estimator_weight * misses)\n","            weight /= np.sum(weight)\n","\n","            self.estimators.append(estimator)\n","\n","            return weight, estimator_weight, estimator_error\n","        \n","        for t in range(self.n_estimators):\n","            weight, estimator_weight, estimator_error = boost(X_train, \n","                                                            y_train, weight)\n","            self.estimator_errors[t] = estimator_error\n","            self.estimator_weights[t] = estimator_weight\n","\n","    def predict(self, X):\n","        C = self.classes[:, np.newaxis]\n","\n","        predictions = sum((estimator.predict(X) == C).T * w\n","                            for estimator, w in zip(self.estimators,\n","                                                self.estimator_weights))\n","        predictions /= self.estimator_weights.sum()\n","\n","        return self.classes.take(np.argmax(predictions, axis=1))"]},{"cell_type":"code","execution_count":91,"metadata":{"deletable":false,"editable":false,"id":"jVjqVcv_tmYk","nbgrader":{"cell_type":"code","checksum":"32aff6a2c5ed06a7b34e982fc295d895","grade":true,"grade_id":"cell-88a2903df26757f7","locked":true,"points":4,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["\"\"\"Τεστ ορθής υλοποίησης AdaBoost\"\"\"\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.metrics import accuracy_score\n","\n","ab = AdaBoost(n_estimators=20, estimator=DecisionTreeClassifier(max_depth=1, \n","            random_state=1))\n","ab.fit(X_train, y_train)\n","assert round(accuracy_score(ab.predict(X_test), y_test), 4) == 0.9591\n"]},{"cell_type":"code","execution_count":92,"metadata":{"deletable":false,"editable":false,"id":"0QkPMoBNz3T5","nbgrader":{"cell_type":"code","checksum":"71c5a522427b4fc254b65b5d431a767d","grade":false,"grade_id":"cell-4f6f954c3531f480","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# Ίδιο αποτέλεσμα και με τη κλάση της sklearn\n","ab = AdaBoostClassifier(n_estimators=20, algorithm=\"SAMME\", \n","                        estimator=DecisionTreeClassifier(max_depth=1, \n","                                                        random_state=1))\n","ab.fit(X_train, y_train)\n","assert round(accuracy_score(ab.predict(X_test), y_test), 4) == 0.9591"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
